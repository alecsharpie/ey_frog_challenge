{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2022 EY Challenge\n",
    "\n",
    "## Frog Data\n",
    "\n",
    "This notebook demonstrates how to extract frog location data from the Global Biodiversity Information Facility (GBIF). The GBIF occurrence dataset combines data from a wide array of sources, including specimen-related data from natural history museums, observations from citizen science networks, and automated environmental surveys. While these data are constantly changing at GBIF.org, periodic snapshots are taken and made available on the Planetary Computer. For our purposes, we are only interested in a narrow subset of the data relating to frogs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supress Warnings \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import common GIS tools\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import rasterio.features\n",
    "import xrspatial.multispectral as ms\n",
    "\n",
    "# Import Planetary Computer tools\n",
    "import stackstac\n",
    "import pystac\n",
    "import pystac_client\n",
    "import planetary_computer\n",
    "\n",
    "# Plotting tools\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "# Data science tools\n",
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "\n",
    "# Table visualisation tools\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Area definition\n",
    "\n",
    "For this demonstration, we will constrain our search to frogs in the Richmond area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Richmond\n",
    "min_lon, min_lat = (150.62, -33.69)  # Lower-left corner\n",
    "max_lon, max_lat = (150.83, -33.48)  # Upper-right corner\n",
    "bbox = (min_lon, min_lat, max_lon, max_lat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch GBIF dataset\n",
    "\n",
    "Now we query the Planetary Computer for the GBIF data. We do not need to specify a query region as the dataset  stores snapshots of a more dynamic collection of datasets, hence we only access one item. We will choose the latest snapshot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stac = pystac_client.Client.open(\"https://planetarycomputer.microsoft.com/api/stac/v1\")\n",
    "\n",
    "search = stac.search(\n",
    "    bbox=bbox,\n",
    "    collections=[\"gbif\"],\n",
    "    # query={\"order\": {\"eq\": 'Anura'}},\n",
    "    \n",
    ")\n",
    "\n",
    "gbif_items = search.get_all_items()\n",
    "print('Number of GBIF scenes for given region:',len(gbif_items))\n",
    "for item in gbif_items:\n",
    "    print(item.id)\n",
    "    \n",
    "# Take latest\n",
    "gbif = gbif_items[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is good practice to sign the data items to avoid any authentication issues when querying the Planetary Computer. \n",
    "\n",
    "The GBIF data is very large, and is therefore spread out over 1050 partitions. We can set up a Dask dataframe to interface with the STAC API, allowing us to create a query workflow and load in the data a partition at a time. The following steps defines the Dask dataframe and instructs it on the queries to perform upon loading in a partition, namely to filter for frogs (`order == \"Anura\"`) and to only take those frogs in the Richmond bounding box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take most recent. Sign it too.\n",
    "gbif = planetary_computer.sign(gbif_items[0])\n",
    "gbif_data_asset = gbif.assets['data']\n",
    "\n",
    "\n",
    "df = (\n",
    "    dd.read_parquet(\n",
    "        gbif_data_asset.href,\n",
    "        storage_options=gbif_data_asset.extra_fields[\"table:storage_options\"],\n",
    "        dataset={\"require_extension\": None},\n",
    "    )\n",
    "    [['eventdate', 'order', 'decimallatitude', 'decimallongitude']]\n",
    "    .query(\"order == 'Anura'\")\n",
    ")\n",
    "# Filter for the bounding box\n",
    "df = df[\n",
    "    (df.decimallatitude < max_lat) & \n",
    "    (df.decimallatitude > min_lat) &\n",
    "    (df.decimallongitude < max_lon) & \n",
    "    (df.decimallongitude > min_lon)\n",
    "]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that repeats the above cell\n",
    "# Solves an authentication issue that happens when the extraction is long\n",
    "def resign_planetary_computer():\n",
    "    global gbif_items\n",
    "    # Take most recent. Sign it too.\n",
    "    gbif = planetary_computer.sign(gbif_items[0])\n",
    "    gbif_data_asset = gbif.assets['data']\n",
    "\n",
    "\n",
    "    df = (\n",
    "        dd.read_parquet(\n",
    "            gbif_data_asset.href,\n",
    "            storage_options=gbif_data_asset.extra_fields[\"table:storage_options\"],\n",
    "            dataset={\"require_extension\": None},\n",
    "        )\n",
    "        [['eventdate', 'order', 'decimallatitude', 'decimallongitude']]\n",
    "        .query(\"order == 'Anura'\")\n",
    "    )\n",
    "    # Filter for the bounding box\n",
    "    df = df[\n",
    "        (df.decimallatitude < max_lat) & \n",
    "        (df.decimallatitude > min_lat) &\n",
    "        (df.decimallongitude < max_lon) & \n",
    "        (df.decimallongitude > min_lon)\n",
    "    ]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract data\n",
    "\n",
    "Finally, we are able to extract the data one partition at a time. To save time, we only extract about 10% of the partitions (randomly with probability 0.1). When the extraction is complete, we save the file to csv format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(420)\n",
    "\n",
    "frogs = pd.DataFrame()\n",
    "for i in range(df.npartitions):\n",
    "    if np.random.random() < 0.1:\n",
    "        print(f'Taking {i+1} of {df.npartitions}')\n",
    "        try:\n",
    "            frogs = frogs.append(df.get_partition(i).compute())\n",
    "        except:\n",
    "            df = resign_planetary_computer()\n",
    "            frogs = frogs.append(df.get_partition(i).compute())\n",
    "        print(f'Frogs found so far: {len(frogs)}')\n",
    "\n",
    "# Save to file\n",
    "(\n",
    "    frogs\n",
    "    .reset_index(drop=True)\n",
    "    .assign(\n",
    "        occurrenceStatus = 1\n",
    "    )\n",
    "    .rename(columns={'eventdate':'eventDate', 'decimallatitude':'decimalLatitude', 'decimallongitude':'decimalLongitude'})\n",
    "    .drop('order', 1)\n",
    "    .to_csv(\"richmond_frogs.csv\", index=None)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the extraction is complete, we are left with a table containing the geolocations of frog sightings in Richmond. These data should be used as the ground truth of your algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(\"richmond_frogs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ey_ds",
   "language": "python",
   "name": "conda-env-ey_ds-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
