{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44e516e8-c67d-4f27-8353-7f749f2eed0a",
   "metadata": {},
   "source": [
    "# 2022 EY Data Science Challenge\n",
    "## Model Building\n",
    "Now that we have our response variable and a few predictor variables, we can begin developing a prediction model. The first step is to collect all predictor variables and their response variable (frog or no frog) into one dataframe, ready for model training. Once this dataframe is created, we can use it to train a machine learning model, before visualising the model over the entire Richmond area to discover frog habitats. Finally, we will test our model's accuracy using cross-validation, before learning how to run the model over a different region.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5652ce4c-c75d-4484-a43a-31ffcf7a2b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supress Warnings \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Plotting tools\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data science tools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Geospatial tools\n",
    "import geopandas as gpd\n",
    "import contextily as cx\n",
    "from shapely.geometry import Point, Polygon\n",
    "import xarray as xr\n",
    "import rasterio.features\n",
    "import rasterio as rio\n",
    "# import xrspatial.multispectral as ms\n",
    "\n",
    "# API tools\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# Import Planetary Computer tools\n",
    "import stackstac\n",
    "import pystac\n",
    "import pystac_client\n",
    "import planetary_computer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639ef421-3dea-4894-a605-d1695657335b",
   "metadata": {},
   "source": [
    "### Training a Model for Richmond, NSW\n",
    "\n",
    "For this demonstration, we will constrain our search to frogs in the Richmond NSW area. We will use be using the predictor variables and frog data we generated in prior notebooks, read in from `.nc` files. Namely: \n",
    "1. `richmond_frogs.csv`\n",
    "2. `jrc_mosaic_sample.nc`\n",
    "3. `S2_mosaic_sample.nc`\n",
    "\n",
    "In a later section, we will write functions that can fetch the required data from an arbitrary location, as defined by a bounding box in the same format as the one below. For now, we will continue with the Richmond area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d75a397-ae04-48ae-89bd-406a209be3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Richmond, NSW\n",
    "min_lon, min_lat = (150.62, -33.69)  # Lower-left corner\n",
    "max_lon, max_lat = (150.83, -33.48)  # Upper-right corner\n",
    "bbox = (min_lon, min_lat, max_lon, max_lat)\n",
    "\n",
    "# Plot map of region\n",
    "crs = {'init':'epsg:4326'}\n",
    "fig, ax = plt.subplots(figsize = (7, 7))\n",
    "ax.scatter(x=[min_lon, max_lon], y=[min_lat, max_lat], alpha=0)\n",
    "cx.add_basemap(ax, crs=crs)\n",
    "ax.set_title('Richmond, NSW')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0bba6da-6e94-4b01-ae6f-90dab7af487e",
   "metadata": {},
   "source": [
    "#### Creating the Dataframe\n",
    "\n",
    "\n",
    "##### Loading Frog Response Variable\n",
    "To train a model, we must first join all predictor variables onto the frog dataframe. The `key` field generated below will make joining easier.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b85189-ff9c-4cae-88d9-9486d17c687a",
   "metadata": {},
   "outputs": [],
   "source": [
    "frog_data = (\n",
    "    pd.read_csv('richmond_frogs.csv')\n",
    "    .reset_index()\n",
    "    .rename(columns={'index':'key'})\n",
    ")\n",
    "frog_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e07003-0994-4bc2-bde3-e8e1274f2277",
   "metadata": {},
   "source": [
    "##### Loading in Predictor Variables\n",
    "\n",
    "For this example, we will be combining two datasets for use in our model, JRC and Sentinal-2. To simplify the model, and to assist in later visualisations, we first resize all datasets by a factor of 10. This step will produce the same outcome as if we queried the planetary computer with a resolution of 100 instead of 10. All predictor datasets are then collected in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82fdf9d-6a43-4f3c-9735-3a659cfcdc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "coarsness_factor = np.array([10, 10])\n",
    "\n",
    "filenames = ['jrc_mosaic_sample.nc', 'S2_mosaic_sample.nc']\n",
    "\n",
    "all_datasets = []\n",
    "\n",
    "for file in filenames:\n",
    "    print(f'loading {file}')\n",
    "    \n",
    "    # Read in netcdf4 file to xarray\n",
    "    data = xr.open_dataset(file).load().to_array()\n",
    "    \n",
    "    # Remove time dimension\n",
    "    data = data[0]\n",
    "    \n",
    "    # Reduce dimensions by a factor of 10\n",
    "    print(f\"Shape before (band, y, x): {data.shape}\")\n",
    "    data = data.coarsen(x=coarsness_factor[0], y=coarsness_factor[1], boundary='pad').mean()\n",
    "    print(f\"Shape after (band, y, x): {data.shape}\\n\")\n",
    "    \n",
    "    # Collect all datasets in a list\n",
    "    all_datasets.append(data)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0774bc5-4dff-4812-a1c6-70768abca0c9",
   "metadata": {},
   "source": [
    "##### Joining Features to Response Variable\n",
    "\n",
    "Now that we have read in and tidied up our predictor variables of JRC and Sentinel-2, we now need to join them onto the response variable of frogs. To do this, we loop through the frog occurrence data and assign each frog occurrence the closest pixel values from each of the predictor variables based on the geo-coordinates. The `sel` method of the xarray dataarray comes in handy here. We write this as a function so that it can be reused later in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926ac1e6-ff3d-41b3-a0ba-5da2acd2e832",
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_features(model_data, all_datasets):\n",
    "    \"\"\"Joins the features from each feature dataset onto each response variable. \n",
    "\n",
    "    Arguments:\n",
    "    model_data -- dataframe containing the response variable along with [\"decimalLongitude\", \"decimalLatitude\", \"key\"]\n",
    "    all_datasets -- list of feature datasets stored as xarray dataarrays, indexed with geocoordinates\n",
    "    \"\"\"\n",
    "    for i, data in enumerate(all_datasets):\n",
    "        # For each latitude and longitude coordinate, find the nearest predictor variable pixel values\n",
    "        data_per_point = pd.DataFrame()\n",
    "        for j, (lon, lat, key) in enumerate(zip(model_data.decimalLongitude, model_data.decimalLatitude, model_data.key)):\n",
    "            # Print out some progress markers\n",
    "            if j==0:\n",
    "                print(f\"Joining dataset {i+1} of {len(all_datasets)}\")\n",
    "            if (j+1)%500==0:\n",
    "                print(f\"{j+1} of {len(frog_data)}\")\n",
    "\n",
    "            # Get the predictor pixel at the site of the frog occurrence\n",
    "            nearest_point = data.sel(x=lon, y=lat, method=\"nearest\")\n",
    "            \n",
    "            # Prepare values and columns and save them in a dataframe, saving the join key for later reference\n",
    "            values = np.concatenate((np.squeeze(nearest_point.values), np.array([key])))\n",
    "            columns = list(nearest_point.band.values) + ['key']\n",
    "            data_per_point = data_per_point.append(\n",
    "                pd.DataFrame(\n",
    "                    np.array([values]), \n",
    "                    columns=columns\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # Join the predictor variables we just collected back onto the frog data\n",
    "        model_data = model_data.merge(\n",
    "            data_per_point,\n",
    "            on = ['key'],\n",
    "            how = 'inner'\n",
    "        )\n",
    "        \n",
    "    return model_data\n",
    "\n",
    "model_data = join_features(frog_data, all_datasets)\n",
    "model_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb154fe-e72c-4e0f-b4ea-5a20384de3f5",
   "metadata": {},
   "source": [
    "#### Model Training\n",
    "\n",
    "Now that we have the data in a format appropriate for machine learning, we can begin training a model. For this demonstration notebook, we will use a basic logistic regression model from the [scikit-learn](https://scikit-learn.org/stable/) library. This library offers a wide range of other models, each with the capacity for extensive parameter tuning and customisation capabilities.\n",
    "\n",
    "Scikit-learn models require separation of predictor variables and the response variable. We store the predictor variables in dataframe `X` and the response in the array `y`. We must make sure to drop the response variable from `X`, otherwise the model will have the answers! It also doesn't make sense to use latitude and longitude as predictor variables in such a confined area, so we drop those too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9dd94e-ba93-4667-897b-3923d3712567",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "full_model = LogisticRegression()\n",
    "# Separate the predictor variables from the response\n",
    "X = (\n",
    "    model_data\n",
    "    .drop(['key', 'decimalLongitude', 'decimalLatitude', 'occurrenceStatus'], 1)\n",
    ")\n",
    "y = model_data.occurrenceStatus\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08cb6222-b558-4314-bf1f-2b64dfd9d3dd",
   "metadata": {},
   "source": [
    "For now, we will train the model using all of our training data. Hence, this section will only reflect the in-sample performance of our model, and not the out-of-sample performance. Out-of-sample performance is crucial in estimating how a model will perform in a real world environment. We will attempt to evaluate the out-of-sample performance of this model in a later section, but for now we can have some fun visualising the in-sample performance for Richmond, NSW."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15029b33-daf5-4c4b-8099-de434c74600d",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa51835e-6061-4db0-9ad1-3a2510c6b271",
   "metadata": {},
   "source": [
    "#### Model Prediction\n",
    "\n",
    "Logistic regression is a machine learning model that estimates the probability of a binary response variable. In our case, the model will output the probability of a frog being present at a given location. To visualise this, we need to understand that each pixel on our satellite image has an associated vector of predictor variable values, in this case 10 bands from JRC and Sentinel-2 data. Thus, we can associate a 10 band image with any satellite image. For each of those 10 band pixels, we can use our logistic regression model to output the probability of a frog being found there. Finally, we can visualise this as a heatmap which will show regions that our model thinks are likely frog habitats.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ca1d86-7823-4f13-bb8b-b6ae006592a6",
   "metadata": {},
   "source": [
    "##### Heat Map\n",
    "As touched on above, to create a probability heat map of the Richmond area we must first generate the 10 band image that our model will use to predict probabilities. To do this, we write two functions. The first function is called `create_predictor_image` and takes in a list of predictor datasets, concatenates their bands together and outputs an xarray of 10-band pixel values.\n",
    "\n",
    "The second function is called `predict_frogs`, which takes in the multi-band image outputted by the `create_predictor_image` function as well as the trained model and returns the predictions for each pixel value. Firstly, the $x$ and $y$ indexes in the predictor image are stacked into one multi-index $z=(x, y)$ to produce an $10\\times n$ array, which is the format required to feed into our logistic regression model. Then, the array is fed into the model, returning the model's predictions for the frog likelihood at each pixel. The predicted probabilities are then indexed by the same multi-index $z$ as before, which allows the array to be unstacked and returned as a one-band image, ready for plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205e9652-9c1a-4a9d-a5a6-f1b705875fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_predictor_image(all_datasets):\n",
    "    \"\"\"Returns one xarray with concatenated bands\n",
    "\n",
    "    Arguments:\n",
    "    all_datasets -- list of xarrays of dimensions, where the ith array has dimensions (k_i, n, m). \n",
    "    \"\"\"\n",
    "    # Combine datasets into one multi-band image\n",
    "    for i, data in enumerate(all_datasets):\n",
    "        print(f\"Combining dataset {i+1} of {len(all_datasets)}\")\n",
    "                \n",
    "        if i == 0:\n",
    "            combined_values = data.values\n",
    "            combined_bands = data.band.values\n",
    "            x_coords = data.x\n",
    "            y_coords = data.y\n",
    "            dims = data.dims\n",
    "        else:\n",
    "            combined_values = np.concatenate((combined_values, data.values), axis=0)\n",
    "            combined_bands = np.concatenate((combined_bands, data.band.values))\n",
    "\n",
    "    predictor_image = xr.DataArray(\n",
    "        data=combined_values,\n",
    "        dims=dims,\n",
    "        coords=dict(\n",
    "            band=combined_bands,\n",
    "            x=x_coords,\n",
    "            y=y_coords\n",
    "        )\n",
    "    )\n",
    "    return predictor_image\n",
    "\n",
    "def predict_frogs(predictor_image, model):\n",
    "    \"\"\"Returns a (1, n, m) xarray where each pixel value corresponds to the probability of a frog occurrence.\n",
    "\n",
    "    Arguments:\n",
    "    predictor_image -- (K, n, m) xarray, where K is the number of predictor variables.\n",
    "    model -- sklearn model with K predictor variables.\n",
    "    \"\"\"\n",
    "    # Stack up pixels so they are in the appropriate format for the model\n",
    "    predictor_image = predictor_image.stack(z=(\"x\", \"y\")).transpose()\n",
    "    \n",
    "    # Calculate probability for each pixel point \n",
    "    probabilities = model.predict_proba(\n",
    "        predictor_image\n",
    "    )\n",
    "\n",
    "    # Just take probability of frog (class=1)\n",
    "    probabilities = probabilities[:,1]\n",
    "\n",
    "    # Add the coordinates to the probabilities, saving them in an xarray\n",
    "    resultant_image = xr.DataArray(\n",
    "        data=probabilities,\n",
    "        dims=['z'],\n",
    "        coords=dict(\n",
    "            z=predictor_image.z\n",
    "        )\n",
    "    )\n",
    "    # Unstack the image\n",
    "    resultant_image = resultant_image.unstack()\n",
    "    return resultant_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd59064-75fd-4fda-b841-b5c1f06affd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine datasets into one multi-band image\n",
    "predictor_image = create_predictor_image(all_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30be5006-f0a6-4bf3-9301-23608fcf5353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate probability for each pixel point \n",
    "resultant_image = predict_frogs(predictor_image, full_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd05212b-01f9-42ed-aad6-596aecdb8369",
   "metadata": {},
   "source": [
    "Now that we have successfully created a one-band image of probabilities, all that is left is to visualise it. To do this, we write a function to plot a heatmap of probabilities. In addition to the heatmap, we will also plot the actual map of the area in question, and the binary classification regions of the probability heatmap. The latter is simply a binary mask of the probability heatmap, 1 where the probability is greater than 0.5 and 0 elsewhere. \n",
    "\n",
    "To help visualise the effectiveness of our model, we plot the frog occurrences over top of each image. This can give us an idea of where our model is doing well, and where it is doing poorly.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61111823-98a5-4260-a86a-94f54f472416",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_heatmap(resultant_image, frog_data, title, crs = {'init':'epsg:4326'}):\n",
    "    \"\"\"Plots a real map, probability heatmap, and model classification regions for the probability image from our model.\n",
    "\n",
    "    Arguments:\n",
    "    resultant_image -- (1, n, m) xarray of probabilities output from the model\n",
    "    frog_data -- Dataframe of frog occurrences, indicated with a 1 in the occurrenceStatus column. \n",
    "                 Must contain [\"occurrenceStatus\", \"decimalLongitude\", \"decimalLatitude\"]\n",
    "    title -- string that will be displayed as the figure title\n",
    "    crs -- coordinate reference system for plotting the real map. Defaults to EPSG:4326.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(1, 3, figsize = (20, 10), sharex=True, sharey=True)\n",
    "    \n",
    "    bbox = [resultant_image.x.min(),resultant_image.x.max(),resultant_image.y.min(),resultant_image.y.max()]\n",
    "\n",
    "    # Plot real map\n",
    "    ax[0].scatter(x=[bbox[0], bbox[1]], y=[bbox[2], bbox[3]], alpha=0)\n",
    "    cx.add_basemap(ax[0], crs=crs)\n",
    "    ax[0].set_title('Real map')\n",
    "\n",
    "    # Plot heatmap from model\n",
    "    heatmap = ax[1].imshow(\n",
    "        resultant_image.transpose(), cmap='PiYG', vmin=0, vmax=1.0, interpolation='none', extent=bbox\n",
    "    )\n",
    "    ax[1].set_title('Model Probability Heatmap')\n",
    "\n",
    "    # Plot binary classification from model\n",
    "    regions = ax[2].imshow(\n",
    "        resultant_image.transpose() > 0.5, cmap='PiYG', vmin=0, vmax=1.0, interpolation='none', extent=bbox\n",
    "    )\n",
    "    ax[2].set_title('Model Classification Regions')\n",
    "\n",
    "    # Plot real frogs\n",
    "    for i, axis in enumerate(ax):\n",
    "        filt = frog_data.occurrenceStatus == 1\n",
    "        axis.scatter(frog_data[filt].decimalLongitude, frog_data[filt].decimalLatitude, color = 'dodgerblue', marker='.', alpha=0.5, label='Frogs' if i==0 else '')\n",
    "\n",
    "    fig.colorbar(heatmap, ax=ax, location = 'bottom', aspect=40)\n",
    "    fig.legend(loc = (0.9, 0.9))\n",
    "    fig.suptitle(title, x=0.5, y=0.9, fontsize=20)\n",
    "    \n",
    "\n",
    "plot_heatmap(resultant_image, frog_data, \"Logistic Regression Model Results - Richmond, NSW\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6938c15-e5eb-46dd-8658-c6bd93267e1b",
   "metadata": {},
   "source": [
    "From the plots above, we can begin to infer . "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08badfa5-98bd-4962-b39a-a19966babfd6",
   "metadata": {},
   "source": [
    "#### In-Sample Evaluation\n",
    "\n",
    "Now that we have visualised the model on the Richmond area, we can calculate a some performance metrics to guage the effectiveness of the model. Again, it must be stressed that this is the in-sample performance - the performance on the training set. Hence, the values will tend to overestimate its performance -  so don't get too excited!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45db621c-d83f-4ffc-ba7e-8579a79da9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score, ConfusionMatrixDisplay\n",
    "\n",
    "predictions = full_model.predict(X)\n",
    "\n",
    "print(f\"F1 Score: {f1_score(y, predictions)}\")\n",
    "print(f\"Accuracy: {accuracy_score(y, predictions)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2ea862-0e9f-469f-8d18-f790cd738304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the results in a confusion matrix\n",
    "disp = ConfusionMatrixDisplay.from_estimator(full_model, X, y, display_labels=['Absent', 'Present'], cmap='Blues')\n",
    "disp.figure_.set_size_inches((7, 7))\n",
    "disp.ax_.set_title('Sentinel-2 and JRC Logistic\\nRegression Model Results')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d77515-bbdc-48aa-b21f-5dd6aff7a52c",
   "metadata": {},
   "source": [
    "#### Out-of-sample evaluation\n",
    "\n",
    "When evaluating a machine learning model, it is essential to correctly and fairly evaluate the model's ability to generalise. This is because models have a tendancy to overfit the dataset they are trained on. To estimate the out-of-sample performance, we will use k-fold cross-validation. This technique involves splitting the training dataset into folds, in this case we will use 10. Each iteration, the model is trained on all but one of the folds, which is reserved for testing. This is repeated until all folds have been left out once. At the end of the process, we will have 10 metrics which can be averaged, giving a more reliable and valid measure of model performance. \n",
    "\n",
    "`Scikit-learn` has built-in functions that can assist in k-fold cross validation. In particular, we will use `StratifiedKFold` to split our data into folds, ensuring there is always a balanced number of frogs and non-frogs in each fold.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41e992d-08cd-4ce6-9330-852263bd0b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "\n",
    "cv_model = LogisticRegression()\n",
    "\n",
    "n_folds = 10\n",
    "\n",
    "skf = StratifiedKFold(n_splits=n_folds, random_state=420, shuffle=True)\n",
    "metrics = {'F1': f1_score, 'Accuracy': accuracy_score}\n",
    "results = {'F1': [], 'Accuracy': []}\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(skf.split(X, y)):\n",
    "    # Split the dataset\n",
    "    print(f\"Fold {i+1} of {n_folds}\")\n",
    "    X_train, X_test = X.loc[train_index], X.loc[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # Fit the model with the training set\n",
    "    cv_model.fit(X_train, y_train)\n",
    "    \n",
    "    predictions = cv_model.predict(X_test)\n",
    "    \n",
    "    for metric, fn in metrics.items():\n",
    "        results[metric].append(fn(y_test, predictions))\n",
    "        \n",
    "        \n",
    "print(f'\\nMetrics averaged over {n_folds} trials:')\n",
    "for metric, result in results.items():\n",
    "    print(f\"{metric}: {np.mean(result).round(2)}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce602e0-0e4f-4f3b-a7a8-1496e83a7268",
   "metadata": {},
   "source": [
    "### Testing model on an arbitrary area \n",
    "\n",
    "Now that we have generated a model using data from one area, how do we apply that model to other areas? To do this, we need to write some functions that will allow us to automatically pull the required data for an arbitrary location. Specifically, we need to be able to pull all the predictor variables for a given location, as well as the actual frog sightings in that location for reference. These functions will reuse much of the code from prior notebooks, so we won't go into too much detail.\n",
    "\n",
    "#### Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91b52bc-5926-4e9d-afe1-e015e33d523f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frogs(bbox, query_params, crs = {'init':'epsg:4326'}, orderKey=\"952\", verbose=False):\n",
    "    # Set query parameters\n",
    "    min_lon, min_lat, max_lon, max_lat = bbox\n",
    "    limit = 300\n",
    "    offset = 0\n",
    "    parameters = {\n",
    "        **query_params,\n",
    "        \"orderKey\":orderKey, # The order Anura (frogs) is indicated by key 952\n",
    "        \"decimalLatitude\":f\"{min_lat},{max_lat}\", # Latitude range\n",
    "        \"decimalLongitude\":f\"{min_lon},{max_lon}\", # Longitude range\n",
    "        \"limit\":limit,\n",
    "        \"offset\":offset\n",
    "    }\n",
    "    \n",
    "    # Query API\n",
    "    frogs = pd.DataFrame()\n",
    "    while True:\n",
    "        # Fetch results\n",
    "        parameters['offset'] = offset\n",
    "        response = requests.get(\"https://api.gbif.org/v1/occurrence/search\", params = parameters).json()\n",
    "        total = response['count']\n",
    "        \n",
    "        # Print progress\n",
    "        print(f\"{offset} of {total}\") if verbose else None\n",
    "        \n",
    "        # Add results to dataframe\n",
    "        frogs = frogs.append(\n",
    "            pd.DataFrame(response['results'])\n",
    "            [[\"decimalLatitude\", \"decimalLongitude\"]]\n",
    "            .assign(\n",
    "                occurrenceStatus = 1\n",
    "            )\n",
    "        )\n",
    "        if response['endOfRecords']:\n",
    "            break\n",
    "        offset += limit\n",
    "        \n",
    "    geo_frogs = gpd.GeoDataFrame(\n",
    "        frogs.reset_index(drop=True), \n",
    "        geometry=gpd.points_from_xy(frogs.decimalLongitude, frogs.decimalLatitude),\n",
    "        crs=crs\n",
    "    )\n",
    "    return geo_frogs\n",
    "\n",
    "\n",
    "# Function to access the planetary computer\n",
    "def get_pc(product, bbox, assets={\"image/tiff\"}, resolution=10, pc_query=None, date_range=None, na_val=None):\n",
    "    # Query the planetary computer\n",
    "    stac = pystac_client.Client.open(\"https://planetarycomputer.microsoft.com/api/stac/v1\")\n",
    "    search = stac.search(\n",
    "        bbox=bbox,\n",
    "        datetime=date_range,\n",
    "        collections=[product],\n",
    "        limit=500,  # fetch items in batches of 500\n",
    "        query=pc_query\n",
    "    )\n",
    "    items = list(search.get_items())\n",
    "    print('This is the number of scenes that touch our region:',len(items))\n",
    "    signed_items = [planetary_computer.sign(item).to_dict() for item in items]\n",
    "\n",
    "    # Define the scale according to our selected crs, so we will use degrees\n",
    "    scale = resolution / 111320.0 # degrees per pixel for crs=4326 \n",
    "        \n",
    "    # Stack up the items returned from the planetary computer\n",
    "    data = (\n",
    "        stackstac.stack(\n",
    "            signed_items,\n",
    "            epsg=4326, # Use common Lat-Lon coordinates\n",
    "            resolution=scale, # Use degrees for crs=4326\n",
    "            bounds_latlon = bbox,\n",
    "            resampling=rio.enums.Resampling.average, # Average resampling method (only required when resolution >10)\n",
    "            chunksize=4096,\n",
    "            assets=assets\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    if na_val is not None:\n",
    "        data = data.where(lambda x: x != na_val, other=np.nan)\n",
    "    \n",
    "    # Median Composite\n",
    "    median = data.median(dim=\"time\", skipna=True).compute()\n",
    "    return median\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1997098-aae9-40fb-8812-ccf8d98e8444",
   "metadata": {},
   "source": [
    "#### Query the Hornsby Area"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097ac600-b16e-4e22-b7b3-ebb0dfaba22c",
   "metadata": {},
   "source": [
    "For this, we will use Hornsby, NSW as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea99aae4-8563-4b1d-85a5-d48a0c1dd1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hornsby\n",
    "min_lon, min_lat = (151.02, -33.75)  # Lower-left corner\n",
    "max_lon, max_lat = (151.13, -33.63)  # Upper-right corner\n",
    "bbox = (min_lon, min_lat, max_lon, max_lat)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (7, 7))\n",
    "ax.scatter(x=[min_lon, max_lon], y=[min_lat, max_lat])\n",
    "cx.add_basemap(ax, crs=crs)\n",
    "ax.set_title(\"Berowra Valley National Park, Hornsby NSW\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96679e57-daa7-40ef-98f5-4a2134fd7f4d",
   "metadata": {},
   "source": [
    "Firstly, we grab all frog occurrences in the area from 1800 until 2022."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be11d7e6-b058-43df-8c5f-0584d8667336",
   "metadata": {},
   "outputs": [],
   "source": [
    "hornsby_frogs = get_frogs(bbox, {\"year\":\"1800,2022\"}, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ecbe983-1545-439a-964e-fa7a9287d6d2",
   "metadata": {},
   "source": [
    "Next, we obtain the predictor variables from the planetary computer, being sure to use a resolution of 100m to ensure the scale aligns with that of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48bc371-c78e-4f1f-886a-b9a99534b793",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define products and query parameters for the planetary computer\n",
    "products = {\n",
    "    'jrc-gsw':{\n",
    "        \"resolution\":100\n",
    "    },\n",
    "    'sentinel-2-l2a':{\n",
    "        \"resolution\":100,\n",
    "        'assets':[\"B04\", \"B03\", \"B02\", \"B08\"],\n",
    "        'pc_query':{\"eo:cloud_cover\": {\"lt\": 20}},\n",
    "        'date_range':\"2020-01-01/2020-12-31\",\n",
    "        'na_val':0\n",
    "    }\n",
    "}\n",
    "\n",
    "all_datasets = []\n",
    "\n",
    "for i, (product, params) in enumerate(products.items()):\n",
    "    print(f'loading {product}')\n",
    "    data = get_pc(product, bbox, **params)\n",
    "    \n",
    "    # Collect all datasets in a list\n",
    "    all_datasets.append(data)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e20e29b-9bee-4255-9be8-e5f735bee471",
   "metadata": {},
   "source": [
    "Finally, we can use the `create_predictor_image` and `predict_frogs` functions from before to create the probability heatmap of the new area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f166278-57d4-47b2-9b60-ed1df953beb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_image = create_predictor_image(all_datasets)\n",
    "resultant_image = predict_frogs(predictor_image, full_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef784656-9801-4192-bdc4-74019637ac53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the image\n",
    "plot_heatmap(resultant_image, hornsby_frogs, \"Logistic Regression Model Results - Hornsby, NSW\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454538c8-6bab-4358-a3f9-b02479c91299",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ey_ds",
   "language": "python",
   "name": "conda-env-ey_ds-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
