{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44e516e8-c67d-4f27-8353-7f749f2eed0a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 2022 EY Data Science Challenge\n",
    "## Model Building - Level 3\n",
    "\n",
    "\n",
    "| Challenge | Locations                     | Spatial Res        | Species | Satellite Data                                                |\n",
    "|-----------|-------------------------------|--------------------|---------|---------------------------------------------------------------|\n",
    "| 1         | Australia                     | Coarse (res=1000)  | Pooled  | Weather Data                                                  |\n",
    "| 2         | Australia, Costa Rica         | Moderate (res=100) | Pooled  | Weather Data, Sentinel-2                                      |\n",
    "|***3***    | Australia, Costa Rica,<br>Europe | Fine (res=10)      | Pooled  | Weather Data, Sentinel-2,<br>Land cover, water extent, elevation |\n",
    "\n",
    "\n",
    "In this notebook, we will develop a model combining all predictor variables explored in prior notebooks at fine spatial resolution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5652ce4c-c75d-4484-a43a-31ffcf7a2b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supress Warnings \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data science\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Image processing\n",
    "from scipy.ndimage import convolve\n",
    "\n",
    "# Geospatial\n",
    "import geopandas as gpd\n",
    "import contextily as cx\n",
    "from shapely.geometry import Point, Polygon\n",
    "import xarray as xr\n",
    "import rasterio.features\n",
    "import rasterio as rio\n",
    "# import xrspatial.multispectral as ms\n",
    "\n",
    "# API\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# Import Planetary Computer\n",
    "import stackstac\n",
    "import pystac\n",
    "import pystac_client\n",
    "import planetary_computer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639ef421-3dea-4894-a605-d1695657335b",
   "metadata": {},
   "source": [
    "### Gathering Data for Richmond, NSW\n",
    "\n",
    "For this demonstration, we will constrain our search to frogs in the Richmond NSW area. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d75a397-ae04-48ae-89bd-406a209be3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Richmond, NSW\n",
    "min_lon, min_lat = (150.62, -33.69)  # Lower-left corner\n",
    "max_lon, max_lat = (150.83, -33.48)  # Upper-right corner\n",
    "bbox = (min_lon, min_lat, max_lon, max_lat)\n",
    "\n",
    "# Plot map of region\n",
    "crs = {'init':'epsg:4326'}\n",
    "fig, ax = plt.subplots(figsize = (7, 7))\n",
    "ax.scatter(x=[min_lon, max_lon], y=[min_lat, max_lat], alpha=0)\n",
    "cx.add_basemap(ax, crs=crs)\n",
    "ax.set_title('Richmond, NSW')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1acd4236-b9a9-4622-9a59-2a3365ae9a01",
   "metadata": {},
   "source": [
    "#### Fetching predictor variables\n",
    "Next, we write a function called `get_pc` that will assist us in grabbing each predictor variable from the planetary computer. It will calculate the median mosaic and return it as an xarray object.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a2c9de-e5ee-44c0-af2d-a2fcc3d99753",
   "metadata": {},
   "source": [
    "Here we define the function that will calculate the slope from the elevation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7da9ea-10ed-418e-a803-ea4db74e0c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fsspec\n",
    "\n",
    "# Function to access the planetary computer\n",
    "def get_pc(product, bbox, assets={\"image/tiff\"}, resolution=10, pc_query=None, date_range=None, na_val=None, resample_dims=():\n",
    "    \"\"\"Return the median mosaic xarray of a specified planetary computer product for a given location\"\"\"\n",
    "    \n",
    "    min_lon, min_lat, max_lon, max_lat = bbox\n",
    "    \n",
    "    \n",
    "    # Handle terraclimate differently\n",
    "    if product == 'terraclimate':\n",
    "        collection = pystac.read_file(\"https://planetarycomputer.microsoft.com/api/stac/v1/collections/terraclimate\")\n",
    "        asset = collection.assets[\"zarr-https\"]\n",
    "        store = fsspec.get_mapper(asset.href)\n",
    "        data = xr.open_zarr(store, **asset.extra_fields[\"xarray:open_kwargs\"])\n",
    "        clipped_data = data.sel(lon=slice(min_lon,max_lon),lat=slice(max_lat,min_lat),time=slice('2015-01-01','2019-12-31'))\n",
    "        parsed_data = clipped_data[['tmax', 'tmin', 'ppt', 'soil']]\n",
    "        return parsed_data.compute()\n",
    "        \n",
    "    # Query the planetary computer\n",
    "    stac = pystac_client.Client.open(\"https://planetarycomputer.microsoft.com/api/stac/v1\")\n",
    "    search = stac.search(\n",
    "        bbox=bbox,\n",
    "        datetime=date_range,\n",
    "        collections=[product],\n",
    "        limit=500,  # fetch items in batches of 500\n",
    "        query=pc_query\n",
    "    )\n",
    "    print(search)\n",
    "    items = list(search.get_items())\n",
    "    print(items)\n",
    "    print('This is the number of scenes that touch our region:',len(items))\n",
    "    signed_items = [planetary_computer.sign(item).to_dict() for item in items]\n",
    "\n",
    "    # Define the scale according to our selected crs, so we will use degrees\n",
    "    scale = resolution / 111320.0 # degrees per pixel for crs=4326 \n",
    "        \n",
    "    # Stack up the items returned from the planetary computer\n",
    "    data = (\n",
    "        stackstac.stack(\n",
    "            signed_items,\n",
    "            epsg=4326, # Use common Lat-Lon coordinates\n",
    "            resolution=scale, # Use degrees for crs=4326\n",
    "            bounds_latlon = bbox,\n",
    "            resampling=rio.enums.Resampling.average, # Average resampling method (only required when resolution >10)\n",
    "            chunksize=4096,\n",
    "            assets=assets\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    if na_val is not None:\n",
    "        data = data.where(lambda x: x != na_val, other=np.nan)\n",
    "    \n",
    "    # Median Composite\n",
    "    median = data.median(dim=\"time\", skipna=True).compute()\n",
    "    return median\n",
    "\n",
    "\n",
    "# Define products and query parameters for the planetary computer\n",
    "products = {\n",
    "    # Weather\n",
    "    \"terraclimate\":{}\n",
    "}\n",
    "\n",
    "def get_predictor_datasets(products, bbox, product_names={\"cop-dem-glo-30\":'elevation',\"io-lulc\":'landcover'}):\n",
    "    \"\"\"Fetch and collect all products\"\"\"\n",
    "    all_datasets = []\n",
    "    for i, (product, params) in enumerate(products.items()):\n",
    "        print(f'loading {product}')\n",
    "        data = get_pc(product, bbox, **params)\n",
    "\n",
    "        # Rename bands of those with only one band (which otherwise defaults to generic 'data')\n",
    "        if data.shape[0] == 1:\n",
    "            if product in product_names.keys():\n",
    "                data = data.assign_coords(band=[product_names[product]])\n",
    "\n",
    "        # Collect all datasets in a list\n",
    "        all_datasets.append(data)\n",
    "\n",
    "        # Calculate the gradient of the elevation data\n",
    "        if product == 'cop-dem-glo-30':\n",
    "            data_elevation = data.squeeze().drop(\"band\")\n",
    "            slope_vals = slope_pct(data_elevation, products[product]['resolution'])\n",
    "            data_slope = xr.DataArray(\n",
    "                np.expand_dims(slope_vals, 0),\n",
    "                coords=dict(\n",
    "                    band=['gradient'],\n",
    "                    y=data_elevation.y,\n",
    "                    x=data_elevation.x\n",
    "                )\n",
    "            )\n",
    "            all_datasets.append(data_slope)\n",
    "            \n",
    "    return all_datasets\n",
    "\n",
    "all_datasets = get_predictor_datasets(products, bbox)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8531886-a1bd-4b79-8edc-c869ae4b3030",
   "metadata": {},
   "source": [
    "#### Stitching together all predictor variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69a695c-60ee-43cf-860b-5878e2979cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_predictor_image(all_datasets):\n",
    "    \"\"\"Returns one xarray with concatenated bands\n",
    "\n",
    "    Arguments:\n",
    "    all_datasets -- list of xarrays of dimensions, where the ith array has dimensions (k_i, n, m). \n",
    "    \"\"\"\n",
    "    # Combine datasets into one multi-band image\n",
    "    for i, data in enumerate(all_datasets):\n",
    "        print(f\"Combining dataset {i+1} of {len(all_datasets)}\")\n",
    "                \n",
    "        if i == 0:\n",
    "            combined_values = data.values\n",
    "            combined_bands = data.band.values\n",
    "            x_coords = data.x\n",
    "            y_coords = data.y\n",
    "            dims = data.dims\n",
    "        else:\n",
    "            combined_values = np.concatenate((combined_values, data.values), axis=0)\n",
    "            combined_bands = np.concatenate((combined_bands, data.band.values))\n",
    "\n",
    "    predictor_image = xr.DataArray(\n",
    "        data=combined_values,\n",
    "        dims=dims,\n",
    "        coords=dict(\n",
    "            band=combined_bands,\n",
    "            x=x_coords,\n",
    "            y=y_coords\n",
    "        )\n",
    "    )\n",
    "    return predictor_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa73df49-fd09-4b47-9d5a-999332f35378",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_image = create_predictor_image(all_datasets)\n",
    "predictor_image.band.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0774bc5-4dff-4812-a1c6-70768abca0c9",
   "metadata": {},
   "source": [
    "##### Joining Features to Response Variable\n",
    "\n",
    "Now that we have read in both our response and predictor variables, we now need to join them onto the response variable of frogs. To do this, we loop through the frog occurrence data and assign each frog occurrence the closest predictor pixel value from each of the predictor variables based on the geo-coordinates. The `sel` method of the xarray dataarray comes in handy here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926ac1e6-ff3d-41b3-a0ba-5da2acd2e832",
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_features(model_data, predictor_image):\n",
    "    \"\"\"Joins the features from each feature dataset onto each response variable. \n",
    "\n",
    "    Arguments:\n",
    "    model_data -- dataframe containing the response variable along with [\"decimalLongitude\", \"decimalLatitude\", \"key\"]\n",
    "    all_datasets -- list of feature datasets stored as xarray dataarrays, indexed with geocoordinates\n",
    "    \"\"\"\n",
    "    # For each latitude and longitude coordinate, find the nearest predictor variable pixel values\n",
    "    data_per_point = pd.DataFrame()\n",
    "    for j, (lon, lat, key) in enumerate(zip(model_data.decimalLongitude, model_data.decimalLatitude, model_data.key)):\n",
    "        # Print out some progress markers\n",
    "        if (j+1)%500==0:\n",
    "            print(f\"{j+1} of {len(model_data)}\")\n",
    "\n",
    "        # Get the predictor pixel at the site of the frog occurrence\n",
    "        nearest_point = predictor_image.sel(x=lon, y=lat, method=\"nearest\")\n",
    "\n",
    "        # Prepare values and columns and save them in a dataframe, saving the join key for later reference\n",
    "        values = np.concatenate((np.squeeze(nearest_point.values), np.array([key])))\n",
    "        columns = list(nearest_point.band.values) + ['key']\n",
    "        data_per_point = data_per_point.append(\n",
    "            pd.DataFrame(\n",
    "                np.array([values]), \n",
    "                columns=columns\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Join the predictor variables we just collected back onto the frog data\n",
    "    model_data = model_data.merge(\n",
    "        data_per_point,\n",
    "        on = ['key'],\n",
    "        how = 'inner'\n",
    "    )\n",
    "        \n",
    "    return model_data\n",
    "\n",
    "model_data = join_features(frog_data, predictor_image)\n",
    "model_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb154fe-e72c-4e0f-b4ea-5a20384de3f5",
   "metadata": {},
   "source": [
    "#### Model Training\n",
    "\n",
    "Now that we have the data in a format appropriate for machine learning, we can begin training a model. For this demonstration notebook, we will use a basic logistic regression model from the [scikit-learn](https://scikit-learn.org/stable/) library. This library offers a wide range of other models, each with the capacity for extensive parameter tuning and customisation capabilities.\n",
    "\n",
    "Scikit-learn models require separation of predictor variables and the response variable. We store the predictor variables in dataframe `X` and the response in the array `y`. We must make sure to drop the response variable from `X`, otherwise the model will have the answers! It also doesn't make sense to use latitude and longitude as predictor variables in such a confined area, so we drop those too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9dd94e-ba93-4667-897b-3923d3712567",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "full_model = LogisticRegression()\n",
    "# Separate the predictor variables from the response\n",
    "X = (\n",
    "    model_data\n",
    "    .drop(['key', 'decimalLongitude', 'decimalLatitude', 'occurrenceStatus', 'geometry'], 1)\n",
    ")\n",
    "y = model_data.occurrenceStatus.astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08cb6222-b558-4314-bf1f-2b64dfd9d3dd",
   "metadata": {},
   "source": [
    "For now, we will train the model using all of our training data. Hence, this section will only reflect the in-sample performance of our model, and not the out-of-sample performance. Out-of-sample performance is crucial in estimating how a model will perform in a real world environment. We will attempt to evaluate the out-of-sample performance of this model in a later section, but for now we can have some fun visualising the in-sample performance for Richmond, NSW."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15029b33-daf5-4c4b-8099-de434c74600d",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa51835e-6061-4db0-9ad1-3a2510c6b271",
   "metadata": {},
   "source": [
    "#### Model Prediction\n",
    "\n",
    "Logistic regression is a machine learning model that estimates the probability of a binary response variable. In our case, the model will output the probability of a frog being present at a given location. To visualise this, we need to understand that each pixel on our satellite image has an associated $k$ dimensional vector of predictor variable values, in this case $k=13$ bands relating to elevation data, landcover, JRC water extent, and Sentinel-2 data. Thus, we can associate a $k$ band image with any satellite image. For each of those $k$ band pixels, we can use our logistic regression model to output the probability of a frog being found there. Finally, we can visualise this as a heatmap which will show regions that our model thinks are likely frog habitats.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ca1d86-7823-4f13-bb8b-b6ae006592a6",
   "metadata": {},
   "source": [
    "##### Heat Map\n",
    "Above, we stitched together each predictor variable into a $k$ band image using the `create_predictor_image` function. Now, we will define another function called `predict_frogs` that will take this image in, along with our logistic regression model, and output the probabilities for each pixel.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205e9652-9c1a-4a9d-a5a6-f1b705875fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_frogs(predictor_image, model):\n",
    "    \"\"\"Returns a (1, n, m) xarray where each pixel value corresponds to the probability of a frog occurrence.\n",
    "    \n",
    "    Takes in the multi-band image outputted by the `create_predictor_image` function as well as the\n",
    "    trained model and returns the predictions for each pixel value. Firstly, the $x$ and $y$ indexes\n",
    "    in the predictor image are stacked into one multi-index $z=(x, y)$ to produce an $k\\times n$\n",
    "    array, which is the format required to feed into our logistic regression model. Then, the array\n",
    "    is fed into the model, returning the model's predictions for the frog likelihood at each pixel. \n",
    "    The predicted probabilities are then indexed by the same multi-index $z$ as before, which allows \n",
    "    the array to be unstacked and returned as a one-band image, ready for plotting.\n",
    "\n",
    "    Arguments:\n",
    "    predictor_image -- (K, n, m) xarray, where K is the number of predictor variables.\n",
    "    model -- sklearn model with K predictor variables.\n",
    "    \"\"\"\n",
    "    # Stack up pixels so they are in the appropriate format for the model\n",
    "    predictor_image = predictor_image.stack(z=(\"x\", \"y\")).transpose()\n",
    "    \n",
    "    # Calculate probability for each pixel point \n",
    "    probabilities = model.predict_proba(\n",
    "        predictor_image\n",
    "    )\n",
    "\n",
    "    # Just take probability of frog (class=1)\n",
    "    probabilities = probabilities[:,1]\n",
    "\n",
    "    # Add the coordinates to the probabilities, saving them in an xarray\n",
    "    resultant_image = xr.DataArray(\n",
    "        data=probabilities,\n",
    "        dims=['z'],\n",
    "        coords=dict(\n",
    "            z=predictor_image.z\n",
    "        )\n",
    "    )\n",
    "    # Unstack the image\n",
    "    resultant_image = resultant_image.unstack()\n",
    "    return resultant_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30be5006-f0a6-4bf3-9301-23608fcf5353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate probability for each pixel point \n",
    "resultant_image = predict_frogs(predictor_image, full_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd05212b-01f9-42ed-aad6-596aecdb8369",
   "metadata": {},
   "source": [
    "Now that we have successfully created a one-band image of probabilities, all that is left is to visualise it. To do this, we write a function to plot a heatmap of probabilities. In addition to the heatmap, we will also plot the actual map of the area in question, and the binary classification regions of the probability heatmap. The latter is simply a binary mask of the probability heatmap, 1 where the probability is greater than 0.5 and 0 elsewhere. \n",
    "\n",
    "To help visualise the effectiveness of our model, we plot the frog occurrences over top of each image. This can give us an idea of where our model is doing well, and where it is doing poorly.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61111823-98a5-4260-a86a-94f54f472416",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_heatmap(resultant_image, frog_data, title, crs = {'init':'epsg:4326'}):\n",
    "    \"\"\"Plots a real map, probability heatmap, and model classification regions for the probability image from our model.\n",
    "\n",
    "    Arguments:\n",
    "    resultant_image -- (1, n, m) xarray of probabilities output from the model\n",
    "    frog_data -- Dataframe of frog occurrences, indicated with a 1 in the occurrenceStatus column. \n",
    "                 Must contain [\"occurrenceStatus\", \"decimalLongitude\", \"decimalLatitude\"]\n",
    "    title -- string that will be displayed as the figure title\n",
    "    crs -- coordinate reference system for plotting the real map. Defaults to EPSG:4326.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(1, 3, figsize = (20, 10), sharex=True, sharey=True)\n",
    "    \n",
    "    bbox = [resultant_image.x.min(),resultant_image.x.max(),resultant_image.y.min(),resultant_image.y.max()]\n",
    "\n",
    "    # Plot real map\n",
    "    ax[0].scatter(x=[bbox[0], bbox[1]], y=[bbox[2], bbox[3]], alpha=0)\n",
    "    cx.add_basemap(ax[0], crs=crs)\n",
    "    ax[0].set_title('Real map')\n",
    "\n",
    "    # Plot heatmap from model\n",
    "    heatmap = ax[1].imshow(\n",
    "        resultant_image.transpose(), cmap='PiYG', vmin=0, vmax=1.0, interpolation='none', extent=bbox\n",
    "    )\n",
    "    ax[1].set_title('Model Probability Heatmap')\n",
    "\n",
    "    # Plot binary classification from model\n",
    "    regions = ax[2].imshow(\n",
    "        resultant_image.transpose() > 0.5, cmap='PiYG', vmin=0, vmax=1.0, interpolation='none', extent=bbox\n",
    "    )\n",
    "    ax[2].set_title('Model Classification Regions')\n",
    "\n",
    "    # Plot real frogs\n",
    "    for i, axis in enumerate(ax):\n",
    "        filt = frog_data.occurrenceStatus == 1\n",
    "        axis.scatter(frog_data[filt].decimalLongitude, frog_data[filt].decimalLatitude, color = 'dodgerblue', marker='.', alpha=0.5, label='Frogs' if i==0 else '')\n",
    "\n",
    "    fig.colorbar(heatmap, ax=ax, location = 'bottom', aspect=40)\n",
    "    fig.legend(loc = (0.9, 0.9))\n",
    "    fig.suptitle(title, x=0.5, y=0.9, fontsize=20)\n",
    "    \n",
    "\n",
    "plot_heatmap(resultant_image, frog_data, \"Logistic Regression Model Results - Richmond, NSW\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08badfa5-98bd-4962-b39a-a19966babfd6",
   "metadata": {},
   "source": [
    "#### In-Sample Evaluation\n",
    "\n",
    "Now that we have visualised the model on the Richmond area, we can calculate some performance metrics to guage the effectiveness of the model. Again, it must be stressed that this is the in-sample performance - the performance on the training set. Hence, the values will tend to overestimate its performance -  so don't get too excited!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45db621c-d83f-4ffc-ba7e-8579a79da9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score, ConfusionMatrixDisplay\n",
    "\n",
    "predictions = full_model.predict(X)\n",
    "\n",
    "print(f\"F1 Score: {f1_score(y, predictions)}\")\n",
    "print(f\"Accuracy: {accuracy_score(y, predictions)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2ea862-0e9f-469f-8d18-f790cd738304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the results in a confusion matrix\n",
    "disp = ConfusionMatrixDisplay.from_estimator(full_model, X, y, display_labels=['Absent', 'Present'], cmap='Blues')\n",
    "disp.figure_.set_size_inches((7, 7))\n",
    "disp.ax_.set_title('Sentinel-2 and JRC Logistic\\nRegression Model Results')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d77515-bbdc-48aa-b21f-5dd6aff7a52c",
   "metadata": {},
   "source": [
    "#### Out-of-sample evaluation\n",
    "\n",
    "When evaluating a machine learning model, it is essential to correctly and fairly evaluate the model's ability to generalise. This is because models have a tendancy to overfit the dataset they are trained on. To estimate the out-of-sample performance, we will use k-fold cross-validation. This technique involves splitting the training dataset into folds, in this case we will use 10. Each iteration, the model is trained on all but one of the folds, which is reserved for testing. This is repeated until all folds have been left out once. At the end of the process, we will have 10 metrics which can be averaged, giving a more reliable and valid measure of model performance. \n",
    "\n",
    "`Scikit-learn` has built-in functions that can assist in k-fold cross validation. In particular, we will use `StratifiedKFold` to split our data into folds, ensuring there is always a balanced number of frogs and non-frogs in each fold.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41e992d-08cd-4ce6-9330-852263bd0b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "cv_model = LogisticRegression()\n",
    "\n",
    "n_folds = 10\n",
    "\n",
    "skf = StratifiedKFold(n_splits=n_folds, random_state=420, shuffle=True)\n",
    "metrics = {'F1': f1_score, 'Accuracy': accuracy_score}\n",
    "results = {'F1': [], 'Accuracy': []}\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(skf.split(X, y)):\n",
    "    # Split the dataset\n",
    "    print(f\"Fold {i+1} of {n_folds}\")\n",
    "    X_train, X_test = X.loc[train_index], X.loc[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # Fit the model with the training set\n",
    "    cv_model.fit(X_train, y_train)\n",
    "    \n",
    "    predictions = cv_model.predict(X_test)\n",
    "    \n",
    "    for metric, fn in metrics.items():\n",
    "        results[metric].append(fn(y_test, predictions))\n",
    "        \n",
    "        \n",
    "print(f'\\nMetrics averaged over {n_folds} trials:')\n",
    "for metric, result in results.items():\n",
    "    print(f\"{metric}: {np.mean(result).round(2)}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce602e0-0e4f-4f3b-a7a8-1496e83a7268",
   "metadata": {},
   "source": [
    "### Testing model on an arbitrary area \n",
    "\n",
    "Now that we have generated a model using data from one area, how do we apply that model to other areas? To do this, we need to write some functions that will allow us to automatically pull the required data for an arbitrary location. Specifically, we need to be able to pull all the predictor variables for a given location, as well as the actual frog sightings in that location for reference. These functions will reuse much of the code from prior notebooks, so we won't go into too much detail.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1997098-aae9-40fb-8812-ccf8d98e8444",
   "metadata": {},
   "source": [
    "#### Query the Hornsby Area"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097ac600-b16e-4e22-b7b3-ebb0dfaba22c",
   "metadata": {},
   "source": [
    "For this, we will use Hornsby, NSW as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea99aae4-8563-4b1d-85a5-d48a0c1dd1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hornsby\n",
    "min_lon, min_lat = (151.02, -33.75)  # Lower-left corner\n",
    "max_lon, max_lat = (151.13, -33.63)  # Upper-right corner\n",
    "bbox = (min_lon, min_lat, max_lon, max_lat)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (7, 7))\n",
    "ax.scatter(x=[min_lon, max_lon], y=[min_lat, max_lat])\n",
    "cx.add_basemap(ax, crs=crs)\n",
    "ax.set_title(\"Berowra Valley National Park, Hornsby NSW\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96679e57-daa7-40ef-98f5-4a2134fd7f4d",
   "metadata": {},
   "source": [
    "Firstly, we grab all frog occurrences in the area from 1800 until 2022."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be11d7e6-b058-43df-8c5f-0584d8667336",
   "metadata": {},
   "outputs": [],
   "source": [
    "hornsby_frogs = get_frogs(bbox, {\"year\":\"1800,2022\"}, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ecbe983-1545-439a-964e-fa7a9287d6d2",
   "metadata": {},
   "source": [
    "Next, we obtain the predictor variables from the planetary computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48bc371-c78e-4f1f-886a-b9a99534b793",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define products and query parameters for the planetary computer\n",
    "hornsby_datasets = get_predictor_datasets(products, bbox)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e20e29b-9bee-4255-9be8-e5f735bee471",
   "metadata": {},
   "source": [
    "Finally, we can use the `create_predictor_image` and `predict_frogs` functions from before to create the probability heatmap of the new area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f166278-57d4-47b2-9b60-ed1df953beb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "hornsby_predictor_image = create_predictor_image(hornsby_datasets)\n",
    "hornsby_resultant_image = predict_frogs(hornsby_predictor_image, full_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef784656-9801-4192-bdc4-74019637ac53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the image\n",
    "plot_heatmap(hornsby_resultant_image, hornsby_frogs, \"Logistic Regression Model Results - Hornsby, NSW\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454538c8-6bab-4358-a3f9-b02479c91299",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ey_ds",
   "language": "python",
   "name": "conda-env-ey_ds-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
